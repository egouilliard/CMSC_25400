{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eDY78I8-wXdy",
      "metadata": {
        "id": "eDY78I8-wXdy"
      },
      "source": [
        "# Project 1: Regression and Model Selection\n",
        "\n",
        "In this project, you will construct a linear regression model, and apply it to a synthetic task as well as a real-world house value prediction task.\n",
        "\n",
        "## Objectives\n",
        "Your goal in this project is to get comfortable in implementing a complete supervised learning pipeline (in Python). To complete this project, you should understand the following:\n",
        "\n",
        "* How to use basic math and machine learning modules in python such as numpy, matplotlib, and sklearn (*_You are encouraged to use numpy to vectorize array operations [see link](https://numpy.org/doc/stable//user/absolute_beginners.html#basic-array-operations)_*)\n",
        "* How to train a regression model *from scratch*\n",
        "* How to implement the *gradient descent* algorithm for iterative model update\n",
        "* How to perform model section when facing multiple choices\n",
        "* How to evaluate the test results and visualize the outcome of an ML model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a0af1538",
      "metadata": {},
      "source": [
        "## Deliverable\n",
        "* Project report/writeup: A `project1_report_lastname.pdf` file with corresponding plots and results for the project. Follow the `Project 1 - Report (Individual Submission)` link on Gradescope to upload this file. The project report should include a brief justification of your solution at a high-level, e.g., using any relevant explanations, equations, or pictures that help to explain your solution. You should also describe what your code does, e.g. using a couple of sentences per function to describe your code structure. \n",
        "\n",
        "* Source code: A `project1_src_lastname1[_lastname2].ipynb` (or `.zip`) file with a working copy of your solutions compiled in a Jupyter notebook. Follow the `Project 1 - Source Code (Group Submission)` link to upload this file.\n",
        "\n",
        "\n",
        "## Logistics\n",
        "\n",
        "* You can work in groups of 1-2 students for each course project, and it's your responsibility to find a group (e.g. use Ed Discussion). \n",
        "* Every member of a group must complete and submit the project report/writeup individually. While the source code can be the same for all group members, the project report needs to be written independently by each person and, thus, should differ among team member and students more generally.\n",
        "* One one group member need to submit the source code. If you submit as a group, make sure to include your teammate in the group submission. Instructions for team submission can be found [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
        "* Grades will be provided based on the individual project report. The source code submission will not be graded, but the teaching staff may check the source files if they see the need for reproducing your results when going through your project report. \n",
        "* Failure to submit the source code will lead to a deduction of points from your total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d-5QFRQs8KMZ",
      "metadata": {
        "id": "d-5QFRQs8KMZ"
      },
      "source": [
        "## Task 1A: (Warmup) Linear Regression on Synthetic Dataset, *from Scratch*\n",
        "\n",
        "### Loading math, machine learning, and visualization modules\n",
        "We start importing some modules and running some magic commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KSOkRgRg8vIe",
      "metadata": {
        "id": "KSOkRgRg8vIe"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# General math and plotting modules.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Machine Learning library. \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_zC9jNFb_mVJ",
      "metadata": {
        "id": "_zC9jNFb_mVJ"
      },
      "source": [
        "### Utility/helper functions for generating synthetic dataset, plots, etc.\n",
        "\n",
        "Below are a few utility functions for generating plots (you don't have to modify these functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9BrgqQPf_ll-",
      "metadata": {
        "id": "9BrgqQPf_ll-"
      },
      "outputs": [],
      "source": [
        "def generate_polynomial_data(num_points, noise, w):\n",
        "    dim = w.size - 1\n",
        "    # Generate feature vector \n",
        "    \n",
        "    # We often use np.random.seed for repeatability; \n",
        "    # remove it if you want to inject randomness into your results \n",
        "    np.random.seed(42)\n",
        "\n",
        "    x = np.random.normal(size=(num_points, 1))\n",
        "    x1 = np.power(x, 0)\n",
        "    for d in range(dim):\n",
        "        x1 = np.concatenate((np.power(x, 1 + d), x1), axis=1)  # X = [x, 1].\n",
        "    y = np.dot(x1, w) + np.random.normal(size=(num_points,)) * noise  # y = Xw + eps\n",
        "\n",
        "    return x1, y\n",
        "\n",
        "def plot_data(X, Y, fig=None, options=dict()):\n",
        "    if fig is None:\n",
        "        fig = plt.subplot(111)\n",
        "    fig.plot(X, Y, options.get('marker', 'b*'), \n",
        "        label=options.get('label', 'Raw data'),\n",
        "        fillstyle=options.get('fillstyle', 'full'),\n",
        "        ms=options.get('size', 8))\n",
        "    process_plot(fig, options)\n",
        "\n",
        "def plot_fit(X, w, fig=None, options=dict()):\n",
        "    if fig is None:\n",
        "        fig = plt.subplot(111)\n",
        "\n",
        "    x_min = np.min(X[:, -2])\n",
        "    x_max = np.max(X[:, -2])\n",
        "    dim = w.size - 1\n",
        "    x_plot = np.reshape(np.linspace(x_min, x_max, 100), [-1, 1])\n",
        "    x1_plot = np.ones_like(x_plot)\n",
        "    for d in range(dim):\n",
        "        x1_plot = np.concatenate((np.power(x_plot, 1 + d), x1_plot), axis=1)\n",
        "\n",
        "    y_plot = np.dot(x1_plot, w)\n",
        "    fig.plot(x_plot, y_plot, 'r-', label=options.get('label', 'Regression fit'))\n",
        "    process_plot(fig, options)\n",
        "\n",
        "def process_plot(fig, options=dict()):\n",
        "    if 'x_label' in options.keys():\n",
        "        fig.set_xlabel(options['x_label'])\n",
        "    if 'y_label' in options.keys():\n",
        "        fig.set_ylabel(options['y_label'])\n",
        "    if 'x_lim' in options.keys():\n",
        "        fig.set_ylim(options['x_lim'])\n",
        "    if 'y_lim' in options.keys():\n",
        "        fig.set_ylim(options['y_lim'])\n",
        "    if 'title' in options.keys():\n",
        "        fig.set_title(options['title'])\n",
        "    if 'legend' in options.keys():\n",
        "        if options['legend']:\n",
        "            fig.legend(loc=options.get('legend_loc', 'best'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sMqfxOF-90Wn",
      "metadata": {
        "id": "sMqfxOF-90Wn"
      },
      "source": [
        "### Data loader\n",
        "\n",
        "Loading and processing dataset (you don't have to modify these functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nbckc1W4-bGt",
      "metadata": {
        "id": "nbckc1W4-bGt"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate the data.\n",
        "num_points = 100  # Number of training points.\n",
        "noise = 0.6  # Noise Level (needed for data generation).\n",
        "\n",
        "w_true = np.array([2, 0.6]) # groundtruth function parameter\n",
        "\n",
        "X, y = generate_polynomial_data(num_points, noise, w_true) # y = X w_true + noise\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=.4, random_state=42)\n",
        "\n",
        "# Plot Data\n",
        "fig = plt.subplot(111);\n",
        "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Generated Data', 'y_lim': [np.min(y)-0.5, np.max(y)+0.5]}\n",
        "plot_data(X[:, 0], y, fig=fig, options=plot_opts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QadXjQtrBHXR",
      "metadata": {
        "id": "QadXjQtrBHXR"
      },
      "source": [
        "### Modeling (30pt)\n",
        "\n",
        "In the following, you will write you own loss function for this linear regression task. This could be later used for the regression applications in Task 1B.\n",
        "\n",
        "\n",
        "### The key ingredient of modeling: risk (i.e. expected loss) function and its gradient\n",
        "\n",
        "We will start by fitting by linear regression a set of data points of the type $D = \\left\\{(\\mathbf{x}, y)_i\\right\\}$, $i \\in \\{1, 2, \\ldots, n\\}$.\n",
        "\n",
        "The objective of linear regression, is to find coefficents $\\hat{w}$ such that the residual between $\\hat{y} = \\hat{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$, and $y$ is small. (Remember that $\\tilde{\\mathbf{x}} = [\\mathbf{x}, 1]$). From now on, $\\mathbf{x}$ will be considered the extended version unless stated otherwise, hence dropping the tilde notation. \n",
        "\n",
        "We consider the ridge regression risk function, defined as \n",
        "$$ R({\\mathbf{w}}) = \\mathbb{E}[(y-{\\mathbf{w}}^\\top x)^2)] +  \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
        "\n",
        "where the expectation is taken over the data generating the distribution of points. As the whole data generating distribution is not known, the expectation is approximated by samples from the **training** set. \n",
        "\n",
        "The risk is approximated by the *empirical risk* as:\n",
        "\n",
        "\n",
        "$$\\hat{R}_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
        "\n",
        "In the following, construct a customized funciton which returns the empirical risk and its gradient at parameter $\\theta$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "W7gfoERbCtRE",
      "metadata": {
        "id": "W7gfoERbCtRE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100,)\n",
            "Loss at initial theta (zeros): 3.4740788381807888\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Constructing the empirical risk function for ridge regression.\n",
        "def lossFunction(theta, X, y, Lambda):\n",
        "    \"\"\"\n",
        "    Take in numpy array of theta, X, and y to return the regularized loss function and gradient\n",
        "    of Ridge regression.\n",
        "    \"\"\"\n",
        "    n = X.shape[0]  # Number of samples\n",
        "\n",
        "    # Predicted values using the current value of theta\n",
        "    y_pred = X.dot(theta).flatten()  # Ensuring y_pred is a 1D array\n",
        "    print(y_pred.shape)\n",
        "\n",
        "    # Residuals between predicted and actual values\n",
        "    residuals = y_pred - y\n",
        "\n",
        "    # Regularized average loss (empirical risk)\n",
        "    regLoss = (1/n) * np.sum(residuals ** 2) + Lambda * np.dot(theta.flatten(), theta.flatten())\n",
        "\n",
        "    # Gradient of the loss function with respect to theta\n",
        "    grad = (2/n) * X.T.dot(residuals) + 2 * Lambda * theta\n",
        "\n",
        "    return regLoss, grad\n",
        "\n",
        "# Initialize fitting parameters\n",
        "initial_theta = np.zeros((X.shape[1], 1))\n",
        "\n",
        "# Set regularization parameter lambda\n",
        "Lambda = 0.1\n",
        "\n",
        "#Compute and display initial loss and gradient for regularized logistic regression\n",
        "emp_risk, grad=lossFunction(initial_theta, X, y, Lambda)\n",
        "print(\"Loss at initial theta (zeros):\",emp_risk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MxRzx3IrVrJ4",
      "metadata": {
        "id": "MxRzx3IrVrJ4"
      },
      "source": [
        "### Training your ridge regressor: Gradient descent (15pt)\n",
        "\n",
        "There are many algorithmic tools for training your classifier. Here, we use the popular gradient descent algorithm:\n",
        "\n",
        "The parameters $\\hat{\\mathbf{w}}$ can be updated via a gradient descent rule: \n",
        "\n",
        "$$ \\hat{\\mathbf{w}}_{k+1} \\gets \\hat{\\mathbf{w}}_k - \\eta_k \\left.\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}} \\right|_{\\mathbf{w}=\\mathbf{w}_k},$$\n",
        "\n",
        "where $\\eta_k$ is a parameter of the algorithm, $k$ is the iteration index, and $\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}}$ is the gradient of the empirical risk function w.r.t. $\\mathbf{w}$.\n",
        "\n",
        "In the *vanilla* gradient descent method, $\\eta(k)=\\eta_0$ is a constant. However other algorithms exists that modify this.\n",
        "\n",
        "The computational complexity of Gradient descent is $O(n_{\\text{iter}} \\cdot  n d)$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NrsxE-mPXUvB",
      "metadata": {
        "id": "NrsxE-mPXUvB"
      },
      "source": [
        "Write a customized function `gradientDescent(X,y,theta,eta,Lambda,tolerance)`\n",
        " which returns an array of empirical risk values, one for each iteration, as well as the final output of the model parameter. \n",
        " \n",
        " Here, `tollerance` specifies the stopping condition: The gradient descent algorithm terminates the observed loss values converges (i.e. two consective losses differ by at most `tollerance`). Hint: the loss should be descending in the loss plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "R1OdRyuoWgZg",
      "metadata": {
        "id": "R1OdRyuoWgZg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60,)\n",
            "(120,)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (120,) (60,) ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m Eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m     30\u001b[0m Tolerance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m---> 32\u001b[0m theta, Loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mgradientDescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe regularized theta using ridge regression:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,theta)\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(Loss_history)\n",
            "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mgradientDescent\u001b[1;34m(X, y, theta, eta, Lambda, tolerance)\u001b[0m\n\u001b[0;32m      8\u001b[0m Loss_history \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Compute the loss and gradient\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43mlossFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Save the current loss\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     Loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n",
            "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mlossFunction\u001b[1;34m(theta, X, y, Lambda)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Residuals between predicted and actual values\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m residuals \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Regularized average loss (empirical risk)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m regLoss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(residuals \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m Lambda \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(theta\u001b[38;5;241m.\u001b[39mflatten(), theta\u001b[38;5;241m.\u001b[39mflatten())\n",
            "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (120,) (60,) "
          ]
        }
      ],
      "source": [
        "def gradientDescent(X,y,theta,eta,Lambda,tolerance):\n",
        "    \"\"\"\n",
        "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
        "    with learning rate of eta\n",
        "    \n",
        "    return theta and the list of the loss of theta during each iteration\n",
        "    \"\"\"\n",
        "    Loss_history =[]\n",
        "    \n",
        "    while True:\n",
        "        # Compute the loss and gradient\n",
        "        loss, grad = lossFunction(theta, X, y, Lambda)\n",
        "        \n",
        "        # Save the current loss\n",
        "        Loss_history.append(loss)\n",
        "        \n",
        "        # Update theta\n",
        "        theta_new = theta - eta * grad\n",
        "        \n",
        "        # Check for convergence (if change in loss is less than tolerance)\n",
        "        if len(Loss_history) > 1 and abs(Loss_history[-2] - Loss_history[-1]) < tolerance:\n",
        "            break\n",
        "        \n",
        "        # Update theta for the next iteration\n",
        "        theta = theta_new\n",
        "    \n",
        "    return theta, Loss_history\n",
        "\n",
        "Eta = 0.01\n",
        "Tolerance = 1e-6\n",
        "\n",
        "theta, Loss_history = gradientDescent(X_train,y_train,initial_theta,Eta,Lambda,Tolerance)\n",
        "print(\"The regularized theta using ridge regression:\\n\",theta)\n",
        "\n",
        "plt.plot(Loss_history)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"$Loss(\\Theta)$\")\n",
        "plt.title(\"Loss function using Gradient Descent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wwWnkqDVeD9n",
      "metadata": {
        "id": "wwWnkqDVeD9n"
      },
      "source": [
        "### Test module (5pt)\n",
        "\n",
        "We still need a method to evaluate the model constructed. Here, we plot the predicted values on the test data, together with the training points.\n",
        "\n",
        "Let's take a look at the final results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YfSEpgt7eMsE",
      "metadata": {
        "id": "YfSEpgt7eMsE"
      },
      "outputs": [],
      "source": [
        "# Plot predicted function\n",
        "fig = plt.subplot(111)\n",
        "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Ridge Regression', 'legend': True,\n",
        "                 'y_lim': [np.min(y)-0.5, np.max(y)+0.5]}\n",
        "\n",
        "plot_data(X_train[:,0], y_train, fig=fig, options=plot_opts)\n",
        "plot_fit(X_test, theta, fig=fig, options=plot_opts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WtqtDFrS1w6Y",
      "metadata": {
        "id": "WtqtDFrS1w6Y"
      },
      "source": [
        "## Task 1B: Real Dataset: House Value Prediction\n",
        "\n",
        "We will use the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) from the scikit-learn package. The task is to predict the house values in California districts given some summary stats about them based on the 1990 census data.\n",
        "\n",
        "The dataset has 8 features: longitudes, latitudes, housing median age, total rooms, total bedrooms, population, households, median income, and median house value. The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($\\$100,000$). We split the dataset as 80\\% for training data and 20\\% for testing data. \n",
        "\n",
        "The following script loads the dataset (using pandas dataframe):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WxDckRfM4Ac_",
      "metadata": {
        "id": "WxDckRfM4Ac_"
      },
      "outputs": [],
      "source": [
        "# python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "california=fetch_california_housing()\n",
        "california_df=pd.DataFrame(california.data,columns=california.feature_names)\n",
        "california_df['Price']=california.target\n",
        "california_df \n",
        "\n",
        "newX=california_df.drop('Price',axis=1)\n",
        "newY=california_df['Price']\n",
        "\n",
        "newX = StandardScaler().fit_transform(newX)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OdD-ghQfqpN-",
      "metadata": {
        "id": "OdD-ghQfqpN-"
      },
      "source": [
        "\n",
        "Following the script below, you will be able to generate the training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374bab52",
      "metadata": {
        "id": "374bab52"
      },
      "outputs": [],
      "source": [
        "N_train = len(newX) * .8\n",
        "\n",
        "np.random.seed(150)\n",
        "msk = np.random.rand(len(newX)) < 0.8\n",
        "\n",
        "X_train, y_train = newX[msk], newY[msk]\n",
        "X_test, y_test = newX[~msk], newY[~msk]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7CCr2O23sHNq",
      "metadata": {
        "id": "7CCr2O23sHNq"
      },
      "source": [
        "### Training and evaluation (25pt)\n",
        "\n",
        "Write a function to fit the Ridge regression on the training data and calculate the MSE on the training set. Choosing $\\lambda$ from $\\{10^{-10}, 10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$, compute the estimate $\\hat{\\mathbf{y}}$ for different values $\\lambda$, and plot the test MSE as a function of $\\lambda$. \n",
        "\n",
        "\n",
        "â—**Note**: You should write your own model fitting and prediction (following the ones you construct in Task 1A). You may call your customized function `lossFunction` and `gradientDescent`. Do not use `sklearn.linear_model` module (i.e. for model fitting and making predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cdd721",
      "metadata": {
        "id": "54cdd721"
      },
      "outputs": [],
      "source": [
        "def train_and_eval( X_train , y_train , X_eval , y_eval , lambda_ ):\n",
        "  \n",
        "  mse = 0\n",
        "  \n",
        "  # add your code to train your model on X_train and evalute (i.e. computing MSE) on X_eval #\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "  return mse\n",
        "\n",
        "\n",
        "weight_list = [1e-10, 1e-6, 1e-4, 0.01, 0.1, 1, 10, 20, 50, 100]\n",
        "result_list = []\n",
        "\n",
        "# compute test MSE\n",
        "for weight in weight_list:\n",
        "    test_mse = train_and_eval ( X_train , y_train , X_test , y_test , weight )\n",
        "    result_list.append ([ test_mse , weight ])\n",
        "    result_array = np.array ( result_list )\n",
        "\n",
        "plt.figure()\n",
        "plt.plot( result_array [: , -1] , result_array [: ,0] , label = 'test mse')\n",
        "plt.xlabel('lambda')\n",
        "plt.ylabel('test mse')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__Sith5GtlSC",
      "metadata": {
        "id": "__Sith5GtlSC"
      },
      "source": [
        "### Model selection via k-fold cross validation (25pt)\n",
        "\n",
        "Implement *10-fold cross validation* on the training set to select $\\lambda$. \n",
        "\n",
        "Plot and compare the MSE on the hold-out set with the true MSE which is computed on the test set. And see how we get to finding the ``best'' $\\lambda$.\n",
        "\n",
        "â—**Note:** For this subproblem, you should write your own function for cross validation; in particular, you should **not** call the existing `sklearn.model_selection` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809941e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation(X_train, y_train, lambda_, k=10):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross validation on X_train and y_train.\n",
        "    \"\"\"\n",
        "    # add your code to perform k-fold cross validation\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    return mse\n",
        "\n",
        "\n",
        "# List of lambdas to try\n",
        "weight_list = [1e-10, 1e-6, 1e-4, 0.01, 0.1, 1, 10, 20, 50, 100]\n",
        "\n",
        "best_lambda = None\n",
        "best_mse = np.inf\n",
        "\n",
        "# Find the best lambda\n",
        "for lambda_ in weight_list:\n",
        "    print(\"cross validation with lambda: \", lambda_)\n",
        "    mse = cross_validation(X_train, y_train, lambda_)\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_lambda = lambda_\n",
        "\n",
        "print(\"Best lambda: \", best_lambda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134d30d0",
      "metadata": {},
      "source": [
        "# Bonus (10pt): Implementing LASSO Regression\n",
        "\n",
        "In this bonus question, you will implement LASSO regression for [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html).\n",
        "\n",
        "Following the previous notation, we consider the Lasso regression risk function, defined as \n",
        "$$ R({\\mathbf{w}}) = \\mathbb{E}[(y-{\\mathbf{w}}^\\top x)^2)] +  \\lambda ||\\mathbf{w}||_1$$\n",
        "\n",
        "where the expectation is taken over the data generating the distribution of points. As the whole data generating distribution is not known, the expectation is approximated by samples from the **training** set. \n",
        "\n",
        "The risk is approximated by the *empirical risk* as:\n",
        "\n",
        "$$\\hat{R}_{\\text{lasso}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda ||\\mathbf{w}||_1$$\n",
        "\n",
        "In the following, construct a customized function which returns the empirical risk and its gradient at parameter $\\theta$:\n",
        "\n",
        "â—**Note**: You should write your own model fitting and prediction (following the ones you construct in Task 1A). You may call your customized function `lossFunctionLASSO`, `gradientDescent` and `train_and_eval`. Do not use `sklearn.linear_model` module (i.e. for model fitting and making predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a89f52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constructing the empirical risk function for LASSO regression.\n",
        "def lossFunctionLASSO(theta, X, y ,Lambda):\n",
        "    \"\"\"\n",
        "    Take in numpy array of theta, X, and y to return the regularize loss function and gradient\n",
        "    of a LASSO regression\n",
        "    \"\"\"\n",
        "    # initialize regularized average loss (empirical risk) and its gradient\n",
        "    regLoss = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    # add your code to compute the LASSO regression risk function and its gradient #\n",
        "    # \n",
        "     \n",
        "    return regLoss, grad\n",
        "\n",
        "def gradientDescent(X,y,theta,eta,Lambda,tolerance,max_iter=1000):\n",
        "    # you can reuse your code from ridge regression and modify it accordingly #\n",
        "    return theta, Loss_history\n",
        "\n",
        "def train_and_eval( X_train , y_train , X_eval , y_eval , lambda_ ):\n",
        "    # you can reuse your code from ridge regression and modify it accordingly #\n",
        "    \n",
        "    return mse\n",
        "\n",
        "\n",
        "weight_list = [1e-10, 1e-6, 1e-4, 0.01, 0.1, 1, 10, 20, 50, 100]\n",
        "result_list = []\n",
        "\n",
        "# compute test MSE\n",
        "for weight in weight_list:\n",
        "    test_mse = train_and_eval ( X_train , y_train , X_test , y_test , weight )\n",
        "    result_list.append ([ test_mse , weight ])\n",
        "    result_array = np.array ( result_list )\n",
        "\n",
        "plt.figure()\n",
        "plt.plot( result_array [: , -1] , result_array [: ,0] , label = 'test mse')\n",
        "plt.xlabel('lambda')\n",
        "plt.ylabel('test mse')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
